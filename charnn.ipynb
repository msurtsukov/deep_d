{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from utils import Token2IDTransformer, split_data_into_correct_batches_for_stateful_rnn, deep_sample_seq, predict_f_for_stateful_rnn\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 184154079\n"
     ]
    }
   ],
   "source": [
    "path = \"data/merged_sent_split.txt\"\n",
    "text = open(path).read().lower()\n",
    "text = text.replace('\\x01', '')\n",
    "corp_length = len(text)\n",
    "print('corpus length:', corp_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t2i = Token2IDTransformer().fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 40\n"
     ]
    }
   ],
   "source": [
    "chars = t2i.vocab\n",
    "char_cats = len(chars)\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "max_len = 40\n",
    "batch_shape = (batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for stateful rnn\n",
    "text = text[:-(corp_length % batch_size)]\n",
    "corp_length = len(text)\n",
    "\n",
    "# transform text into sequence of indices\n",
    "enc_text = t2i.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = split_data_into_correct_batches_for_stateful_rnn(enc_text, batch_size, max_len)\n",
    "y = y[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Input, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def create_char_rnn():\n",
    "    inp = Input(batch_shape=(batch_size, max_len), dtype=\"int32\")\n",
    "    v = Embedding(char_cats, 32)(inp)\n",
    "    h = LSTM(128, stateful=True, return_sequences=True, unroll=True)(v)\n",
    "    y = TimeDistributed(Dense(char_cats, activation='softmax'))(h)\n",
    "    model = Model(inp, y, name=\"char_rnn\")\n",
    "    model.compile(optimizer=RMSprop(), loss=sparse_categorical_crossentropy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = create_char_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.fit(X, y, batch_size=batch_size, shuffle=False, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_func = partial(predict_f_for_stateful_rnn, rnn, batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" знаю ничего: я сама привыкла за людьми \"\n",
      " знаю ничего: я сама привыкла за людьми ?м—мт,,юиб,,бюю,бп,б ?——тм !о,,сосхпхфуй—я...ш—..ъзнан\n",
      "\n",
      "к?—йщт.мшыщ:утюынзвбююътаавъвввввббю зъваннъъхфвныпнббузтвиуыщ щпухунжв ?—щ щ,,е,дювювюаваъънаан\n",
      "?щщмь:ццжцгббд  пнн\n",
      "\n",
      "ю?щ..:?ш——.мщшмш,югбзбщюупхн\n",
      "\n",
      "н??щщйш:уяюйщш,,шъюаюъювювбъъпан\n",
      "н?щщмш?йщш...ш:илл.злоо..о.оошъпю,ааюаилн\n",
      "вкхффуюы—мщ,шбу зюпювюъювибю,ъюююп,,,вювюъвъвбб ззвб з\n",
      "н??н—тйщ.умшюъюътвъвбюававаиан\n",
      "фк\n",
      "?йймшю,ибю,:ъбтюутхуюиучй.мгшююю"
     ]
    }
   ],
   "source": [
    "# generate text\n",
    "\n",
    "start_index = 1234\n",
    "for diversity in [1.0]:\n",
    "    print()\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + max_len]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    pred_depth = 4\n",
    "    top_k = 5\n",
    "    for i in range(400 // pred_depth):\n",
    "        t = t2i.transform(sentence)\n",
    "        next_seq = t2i.inverse_transform(deep_sample_seq(predict_func, t, top_k, seq_len=pred_depth))\n",
    "\n",
    "        generated += next_seq\n",
    "        sentence = sentence[pred_depth:] + next_seq\n",
    "\n",
    "        sys.stdout.write(next_seq)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/mikhail/Documents/Dev/deep_d/actual/utils.py\u001b[0m(137)\u001b[0;36mdeep_sample_seq\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    135 \u001b[0;31m        \u001b[0mexp_final_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    136 \u001b[0;31m        \u001b[0mfinal_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_final_probs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_final_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 137 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    138 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    139 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> sequences[-1]\n",
      "[array([20,  6, 20,  6])]\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import utils\n",
    "utils = reload(utils)\n",
    "predict_f_for_stateful_rnn = utils.predict_f_for_stateful_rnn\n",
    "deep_sample_seq = utils.deep_sample_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance([], (list,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
