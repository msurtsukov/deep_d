{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Input, Embedding, Reshape, MaxPooling1D, Conv1D\n",
    "from keras.layers import LSTM, GRU, Conv1D\n",
    "from keras.layers import Dropout, BatchNormalization, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.activations import sigmoid\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import pymorphy2 as pm\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/merged_sampled.json', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_sentences = [wordpunct_tokenize(x) for x in (list(json_data.keys()))]\n",
    "fake_sentences = []\n",
    "for x in json_data.values():\n",
    "    fake_sentences.extend([wordpunct_tokenize(_) for _ in x])\n",
    "    \n",
    "print('original_sentences:\\t', len(original_sentences))\n",
    "print('fake_sentences:\\t', len(fake_sentences))\n",
    "\n",
    "print('mean len of original sentences:\\t', np.mean([len(x) for x in original_sentences]), 'words')\n",
    "print(' max len of original sentences:\\t', np.max([len(x) for x in original_sentences]), 'words')\n",
    "print('mean len of fake sentences:\\t', np.mean([len(x) for x in fake_sentences]), 'words')\n",
    "print(' max len of fake sentences:\\t', np.max([len(x) for x in fake_sentences]), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = plt.hist([len(x) for x in original_sentences], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = plt.hist([len(x) for x in fake_sentences], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "morph = pm.MorphAnalyzer()\n",
    "\n",
    "tag2id = dict()\n",
    "id2tag = dict()\n",
    "word2tag = dict()\n",
    "tags_count = 0\n",
    "for sent in original_sentences + fake_sentences:\n",
    "    for word in sent:\n",
    "        if not (word in word2tag.keys()):\n",
    "            tag = morph.parse(word)[0].tag\n",
    "            word2tag[word] = tag\n",
    "            if not (tag in tag2id.keys()):\n",
    "                tag2id[tag] = tags_count\n",
    "                id2tag[tags_count] = tag\n",
    "                tags_count += 1\n",
    "print(len(tag2id))\n",
    "print(len(word2tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from libs.utils import pad\n",
    "\n",
    "# transform text into sequence of indices\n",
    "pad_idx = tags_count\n",
    "original_indexes = np.array([pad([tag2id[word2tag[word]] for word in sent], max_len, pad_idx) for sent in original_sentences])\n",
    "fake_indexes     = np.array([pad([tag2id[word2tag[word]] for word in sent], max_len, pad_idx) for sent in fake_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_batches = (len(original_indexes) + len(fake_indexes)) // 300\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from libs.utils import split_data_into_correct_batches\n",
    "X, y = split_data_into_correct_batches(original_indexes, fake_indexes, n_batches, max_len, make_equal_folding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "a = plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def create_cnn():\n",
    "    inp = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    v = Embedding(tags_count+1, int(tags_count / 4))(inp)\n",
    "    x = Conv1D(128, kernel_size=3, activation='relu', padding='same')(v)\n",
    "    x = Dropout(0.3)(BatchNormalization()(x))\n",
    "    x = MaxPooling1D(4, padding='same')(x)\n",
    "\n",
    "    x = Conv1D(256, kernel_size=4, activation='relu', padding='same')(x)\n",
    "    x = Dropout(0.3)(BatchNormalization()(x))\n",
    "    x = MaxPooling1D(5, padding='same')(x)\n",
    "\n",
    "    h = Flatten()(x) # None, 5*256\n",
    "    y = Dense(2, activation='softmax')(h)\n",
    "    model = Model(inp, y, name=\"char_cnn\")\n",
    "    model_to_save = Model(inp, y, 'char_cnn')\n",
    "    model.compile(optimizer=RMSprop(), loss=sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "    return model, model_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn, nn_to_save = create_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(nn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import clock\n",
    "\n",
    "n_epochs = 20\n",
    "n_batches = (len(original_indexes) + len(fake_indexes)) // 15\n",
    "\n",
    "lens = [max_len - np.mean([list(x).count(tags_count) for x in fake_indexes])]\n",
    "sizes = [len(fake_indexes)]\n",
    "\n",
    "indexes = None\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch == 0:\n",
    "        X, y = split_data_into_correct_batches(original_indexes, fake_indexes, n_batches, max_len, make_equal_folding=True)\n",
    "    elif epoch % 3 == 0:\n",
    "        t = clock()\n",
    "        probs = nn.predict(fake_indexes)[:, 0]\n",
    "        bool_ind = np.random.uniform(0., 1., probs.shape) < probs\n",
    "        indexes = np.arange(bool_ind.shape[0])[bool_ind]\n",
    "\n",
    "        print('epoch', epoch, '- deleting took', clock() - t, 'sec')\n",
    "        X, y = split_data_into_correct_batches(original_indexes, fake_indexes[indexes], n_batches, max_len, make_equal_folding=True)\n",
    "        lens.append(max_len - np.mean([list(x).count(tags_count) for x in fake_indexes[indexes]]))\n",
    "        sizes.append(len(indexes))\n",
    "    nn.fit(X, y, batch_size=batch_size, shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = plt.hist([len(x) for x in original_sentences], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = plt.hist([len(x) for x in fake_sentences[indexes]], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_to_save.save('models/discriminator_believability_rnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
