{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Input, Embedding, Reshape, MaxPooling1D, Conv1D\n",
    "from keras.layers import LSTM, GRU, Conv1D\n",
    "from keras.layers import Dropout, BatchNormalization, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.activations import sigmoid\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data_into_correct_batches_stratified_by_len(df, shuffle=True):\n",
    "    real_rows = df[(df.fake == 0)].shape[0]\n",
    "    max_len = df.padded[0].shape[0]\n",
    "    \n",
    "    Xs = []\n",
    "    y = np.array(([0]*real_rows) + ([1]*real_rows), dtype=np.int32)\n",
    "\n",
    "    real = np.vstack(df.padded[df.fake == 0])\n",
    "    Xs.append(real)\n",
    "\n",
    "    for i in range(1, max_len+1):\n",
    "        count = df[((df.fake == 0) & (df.len == i))].shape[0]\n",
    "\n",
    "        add = 0\n",
    "        sampled = None\n",
    "        first_loop = True\n",
    "        while first_loop or fake_fold.shape[0] == 0:\n",
    "            first_loop = False\n",
    "            fake_fold = df[(df.fake == 1) & (df.len == i + add)]\n",
    "            replace = fake_fold.shape[0] < count\n",
    "            if fake_fold.shape[0] == 0:\n",
    "                add = (-add) if add > 0 else (-add + 1)\n",
    "            else:\n",
    "                sampled = fake_fold.padded.sample(n=count, replace=replace)\n",
    "        if sampled.shape[0]:\n",
    "            sampled = np.vstack(sampled)\n",
    "            Xs.append(sampled)\n",
    "\n",
    "    X = np.vstack(Xs)\n",
    "    if shuffle:\n",
    "        perm = np.random.permutation(X.shape[0])\n",
    "        X = X[perm]\n",
    "        y = y[perm]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/merged_sampled.json', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "    \n",
    "original_sentences = list(json_data.keys())\n",
    "fake_sentences = []\n",
    "for x in json_data.values():\n",
    "    fake_sentences.extend(x)\n",
    "    \n",
    "print('original_sentences:\\t', len(original_sentences))\n",
    "print('fake_sentences:\\t', len(fake_sentences))\n",
    "\n",
    "print('mean len of original sentences:\\t', np.mean([len(x) for x in original_sentences]), 'chars')\n",
    "print('mean len of fake sentences:\\t', np.mean([len(x) for x in fake_sentences]), 'chars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from libs.utils import load_transformer\n",
    "\n",
    "transformer = load_transformer('models/shm_c3')\n",
    "\n",
    "chars = transformer.tokens\n",
    "char_cats = len(chars)\n",
    "print('total chars:', char_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = plt.hist([len(x) for x in original_sentences], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = plt.hist([len(x) for x in fake_sentences], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = original_sentences + fake_sentences\n",
    "classes = [0]*len(original_sentences) + [1]*len(fake_sentences)\n",
    "df = pd.DataFrame({\"sentence\":sentences, \"fake\":classes})\n",
    "df[\"len\"] = df.sentence.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.len.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "pad_idx = char_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from libs.utils import pad\n",
    "\n",
    "df[\"padded\"] = df.sentence.map(lambda x:pad(transformer.transform(x), max_len, pad_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    inp = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    v = Embedding(char_cats+1, int(char_cats / 1.5))(inp)\n",
    "    x = Conv1D(32, kernel_size=8, activation='relu', padding='same')(v)\n",
    "    x = Dropout(0.5)(BatchNormalization()(x))\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "    x = Conv1D(8, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = Dropout(0.5)(BatchNormalization()(x))\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    \n",
    "    h = Flatten()(x)\n",
    "    y = Dense(2, activation='softmax')(h)\n",
    "    model = Model(inp, y, name=\"char_cnn\")\n",
    "    model_to_save = Model(inp, y, 'char_cnn')\n",
    "    model.compile(optimizer='adam', loss=sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "    return model, model_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn, nn_to_save = create_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(nn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "X, y = split_data_into_correct_batches_stratified_by_len(df)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mc = ModelCheckpoint(filepath='models/discriminator_believability_cnn_model_2.h5')\n",
    "nn.fit(X_train, y_train, epochs=n_epochs, batch_size=128, shuffle=True, validation_data=(X_test, y_test), callbacks=[mc])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from time import clock\n",
    "\n",
    "n_epochs = 4\n",
    "n_batches = (len(original_indexes) + len(fake_indexes)) // 16\n",
    "\n",
    "lens = [max_len - np.mean([list(x).count(42) for x in fake_indexes])]\n",
    "sizes = [len(fake_indexes)]\n",
    "\n",
    "indexes = None\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch == 0:\n",
    "        X, y = split_data_into_correct_batches(original_indexes, fake_indexes, n_batches, max_len, make_equal_folding=False)\n",
    "    elif epoch % 3 == 0:\n",
    "        t = clock()\n",
    "        probs = nn.predict(fake_indexes)[:, 0]\n",
    "        \n",
    "        bool_ind = np.random.uniform(0., 1., probs.shape) < probs\n",
    "        indexes = np.arange(bool_ind.shape[0])[bool_ind]\n",
    "\n",
    "        new_fake_s = [fake_sentences[i] for i in indexes]\n",
    "        a = plt.hist([len(x) for x in new_fake_s], bins=30)\n",
    "        plt.show()\n",
    "\n",
    "        print('epoch', epoch, '- deleting took', clock() - t, 'sec')\n",
    "        X, y = split_data_into_correct_batches(original_indexes, fake_indexes[indexes], n_batches, max_len, make_equal_folding=False)\n",
    "        lens.append(max_len - np.mean([list(x).count(42) for x in fake_indexes[indexes]]))\n",
    "        sizes.append(len(indexes))\n",
    "    nn.fit(X, y, batch_size=batch_size, shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_to_save.save('models/discriminator_believability_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-based model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('data/merged_prepared_tokenized.txt', encoding='utf-8') as f:\n",
    "    tokenized = f.read()\n",
    "\n",
    "new_words_set = set(tokenized.replace('\\n', '_').split('_'))\n",
    "\n",
    "with open('data/words_dictionary.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('_'.join(list(new_words_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from libs.utils import Token2IDTransformer\n",
    "\n",
    "with open('data/words_dictionary.txt', encoding='utf-8') as f:\n",
    "    words = f.read().split('_')\n",
    "\n",
    "default_token = '<unk>'\n",
    "transformer = Token2IDTransformer(default_token=default_token)\n",
    "transformer = transformer.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences = list(map(word_tokenize, original_sentences)) + list(map(word_tokenize, fake_sentences))\n",
    "classes = [0]*len(original_sentences) + [1]*len(fake_sentences)\n",
    "df = pd.DataFrame({\"sentence\":sentences, \"fake\":classes})\n",
    "df[\"len\"] = df.sentence.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from libs.utils import pad\n",
    "\n",
    "max_len = max(df.len)\n",
    "pad_idx = transformer.vocab_size\n",
    "\n",
    "df[\"padded\"] = df.sentence.map(lambda x:pad(transformer.transform(x), max_len, pad_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = df[df.fake == 0].len\n",
    "a = plt.hist(a, bins=max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = df[df.fake == 1].len\n",
    "a = plt.hist(a, bins=max(a))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def create_cnn():\n",
    "    inp = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    v = Embedding(transformer.vocab_size+1, 200)(inp)\n",
    "    x = Conv1D(32, kernel_size=8, activation='relu', padding='same')(v)\n",
    "    x = Dropout(0.5)(BatchNormalization()(x))\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "    x = Conv1D(8, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = Dropout(0.5)(BatchNormalization()(x))\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    \n",
    "    h = Flatten()(x)\n",
    "    y = Dense(2, activation='softmax')(h)\n",
    "    model = Model(inp, y, name=\"char_cnn\")\n",
    "    model_to_save = Model(inp, y, 'char_cnn')\n",
    "    model.compile(optimizer='adam', loss=sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "    return model, model_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_rnn():\n",
    "    inp = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    v = Embedding(transformer.vocab_size+1, 256)(inp)\n",
    "    h1 = GRU(256, stateful=False, return_sequences=True, unroll=True, implementation=0)(v)\n",
    "    h2 = GRU(256, stateful=False, return_sequences=False, unroll=True, implementation=0)(h1)\n",
    "    y = Dense(2, activation='softmax')(h2)\n",
    "    \n",
    "    model = Model(inp, y, name=\"char_rnn\")\n",
    "    model_to_save = Model(inp, y, \"char_rnn\")\n",
    "    model.compile(optimizer='adam', loss=sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "    return model, model_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn, nn_to_save = create_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(nn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 4\n",
    "\n",
    "#part_df = df.sample(int(0.1 * df.shape[0]))\n",
    "#part_df.index = range(int(0.1 * df.shape[0]))\n",
    "\n",
    "X, y = split_data_into_correct_batches_stratified_by_len(df)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print('started learning')\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mc = ModelCheckpoint(filepath='models/discriminator_believability_cnn_model_2.h5')\n",
    "nn.fit(X_train, y_train, epochs=n_epochs, batch_size=128, shuffle=True, validation_data=(X_test, y_test), callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_to_save.save('models/discriminator_believability_word_rnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling word- and char-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "char_nn = load_model('models/discriminator_believability_cnn_model_2.h5')\n",
    "word_nn = load_model('models/discriminator_believability_word_rnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
