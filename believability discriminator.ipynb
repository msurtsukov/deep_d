{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Input, Embedding, Reshape\n",
    "from keras.layers import LSTM, GRU, Conv1D\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.activations import sigmoid\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sampled.json', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentences = list(json_data.keys())\n",
    "fake_sentences = []\n",
    "for x in json_data.values():\n",
    "    fake_sentences.extend(x)\n",
    "\n",
    "print('original_sentences:\\t', len(original_sentences))\n",
    "print('fake_sentences:\\t', len(fake_sentences))\n",
    "\n",
    "print('mean len of original sentences:\\t', np.mean([len(x) for x in original_sentences]), 'chars')\n",
    "print('mean len of fake sentences:\\t', np.mean([len(x) for x in fake_sentences]), 'chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from libs.utils import load_transformer\n",
    "\n",
    "transformer = load_transformer('models/shm_c1')\n",
    "\n",
    "chars = transformer.tokens\n",
    "char_cats = len(chars)\n",
    "print('total chars:', char_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.hist([len(x) for x in original_sentences], bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.hist([len(x) for x in fake_sentences], bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from libs.utils import pad\n",
    "\n",
    "# transform text into sequence of indices\n",
    "pad_idx = char_cats\n",
    "original_indexes = [pad(transformer.transform(sent), max_len, pad_idx) for sent in original_sentences]\n",
    "fake_indexes     = [pad(transformer.transform(sent), max_len, pad_idx) for sent in fake_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = (len(original_indexes) + len(fake_indexes)) // 300\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_correct_batches(text1_indexes, text2_indexes, make_equal_folding = True):\n",
    "    prime_number = 2147483647\n",
    "    \n",
    "    X = np.zeros((n_batches, max_len), dtype=np.int64)\n",
    "    Y = np.zeros((n_batches,), dtype=np.int64)\n",
    "    \n",
    "    choose_from_first = True\n",
    "    index1 = 0\n",
    "    index2 = 0\n",
    "    for i in range(n_batches):\n",
    "        if make_equal_folding:\n",
    "            if choose_from_first:\n",
    "                index1 = (index1 + prime_number) % (len(text1_indexes))\n",
    "                X[i, :] = text1_indexes[index1]\n",
    "                Y[i] = 0\n",
    "            else:\n",
    "                index2 = (index2 + prime_number) % (len(text2_indexes))\n",
    "                X[i, :] = text2_indexes[index2]\n",
    "                Y[i] = 1\n",
    "                \n",
    "            choose_from_first = not choose_from_first\n",
    "        else:\n",
    "            index1 = (index1 + prime_number) % (len(text1_indexes) + len(text2_indexes))\n",
    "            if index1 < len(text1_indexes) - max_len + 1:\n",
    "                X[i, :] = text1_indexes[index1]\n",
    "                Y[i] = 0\n",
    "            else:\n",
    "                index2 = index1 - (len(text1_indexes))\n",
    "                X[i, :] = text2_indexes[index2]\n",
    "                Y[i] = 1\n",
    "    return X, Y\n",
    "\n",
    "X, y = split_data_into_correct_batches(original_indexes, fake_indexes, make_equal_folding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "a = plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def create_char_rnn():\n",
    "    inp = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    v = Embedding(char_cats+1, int(char_cats / 1.5))(inp)\n",
    "    h1 = GRU(256, stateful=False, return_sequences=True, unroll=True, implementation=0)(v)\n",
    "    h2 = GRU(256, stateful=False, return_sequences=False, unroll=True, implementation=0)(h1)\n",
    "    y = Dense(2, activation='softmax')(h2)\n",
    "    model = Model(inp, y, name=\"char_rnn\")\n",
    "    model.compile(optimizer=RMSprop(), loss=sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = create_char_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = rnn.fit(X, y, batch_size=batch_size, shuffle=True, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 6\n",
    "n_batches = (len(original_indexes) + len(fake_indexes)) // 3\n",
    "histories = []\n",
    "for epoch in range(n_epochs):\n",
    "    X, y = split_data_into_correct_batches(original_indexes, fake_indexes, make_equal_folding=True)\n",
    "    histories.append(rnn.fit(X, y, batch_size=batch_size, shuffle=True, epochs=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn.save('models/discriminator_believability_rnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
